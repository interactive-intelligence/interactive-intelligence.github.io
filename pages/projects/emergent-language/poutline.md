---
layout: default
title: Technical Specifications
parent: Emergent Language
grand_parent: Projects
nav_order: 1
has_children: false
permalink: /projects/emergent-lang/poutline
---

# Project Outline
{: .no_toc }

Detailed ideas and development records
{: .fs-6 .fw-300 }

The description on the home page of the Emergent Language project page gives an overview of the motivation and directions of this project. This page contains additional details on key theoretical advancements and results we have obtained.

---

## Table of Contents
{: .no_toc .text-delta }

1. TOC
{:toc}

---

## Tasks

### Autoencoding with Language Latent Space
Intuitively, we can understand the task of communication as one of autoencoding - one agent (an encoder) collaborates with another agent (a decoder) to structure the latent space such that information can reliably be encoded into it and decoded out of it. Lan et al. say as much in their paper ["On the Spontaneous Emergence of Discrete and Compositional Signals"](https://aclanthology.org/2020.acl-main.433.pdf){:target="_blank"}. 

In autoencoding task, the desired output is identical to the output. The decoder's objective is to perfectly reconstruct the input given a language-like latent representation. This language-like representation must satisfy at least some of the axioms of language (discussed more in-depth in the [Properties of Language](#properties-of-language) section). For the duration of our work on this task, we used a latent space satisfying two of the three core properties - discrete/symbolic and sequential. The intuition is that the encoder and decoder must collaboratively develop an understanding of core features in the image and represent it in important word-like tokens arranged in a meaningful language-like sequence.

Empirically, we observe good performance on the MNIST dataset with a sufficiently large vocabulary size and sequence length.

*A static demonstration of autoencoder reconstructive capabilities and internal + quantized language/'communication'.*

<center>
<img src="https://user-images.githubusercontent.com/73039742/164372921-41279cd8-cfeb-485d-8ec0-5a65206a4924.png" width="60%" />
</center>

*A dynamic demonstration of autoencoder reconstructive capabilities along the temporal/sequential language/'communication' axis.*

<center>
<img src="https://interactive-intelligence.github.io/files/languageProgression.gif" width="60%" />
</center>

*The autoencoder even generates auto-blurred genetalia in infancy!*

<center>
<img src="https://user-images.githubusercontent.com/73039742/164372535-b114c99c-f0d3-4d3c-a31b-f731d68600be.png" width="60%" />
</center>

However, the task of autoencoding with a language latent space becomes practically and philosophically dubious for any dataset more complex than MNIST, like CIFAR-10/100, ImageNet, or a semantic subset like the Stanford Dogs and Cats dataset. There really are only perhaps twenty or less actual digit forms; all remaining digits closely conform to one of these digit forms, since the dataset is low-resolution, centered, and grayscale. A network can reliably autoencode MNIST just be learning these digit form 'templates'. However, for more complex datasets, autoencoding as a task for emergent language becomes philosophically problematic. The purpose of language is not to encode information in a way that optimizes for minimum information loss; rather, it is inherently abstract. However, autoencoding rewards the development of explicit structures and information systems, which is in fact antithetical to the purpose of the project.

### VQ-AE-GAN
The VQ-AE-GAN (Vector-Quantized Autoencoder Generative-Adversarial-Network) system is an attempt to build a task that rewards abstractness by fitting the vector-quantized (i.e. discrete, language-like latent space) autoencoder into a Generative Adversarial Network system. In a traditional GAN system, the generator generates images and the discriminator classifies whether an input was pulled from the dataset or generated by the generator. The generator's goal is to fool the discriminator by maximizing its loss; the discriminator's goal is to minimize its loss. Such an 'adversarial' relationship leads to the development of realstic images that are sophisticated because they demonstrate abstract properties also present in real/true images.

If we replace the generator with a vector-quantized autoencoder (i.e. the same autoencoder architecture used for the previous task), then the vector-quantized autoencoder does not necessarily need to reconstruct the exact input pixel-for-pixel, but rather just develop a reconstruction that seems realistic to a discriminator. The discriminator is an entity that theoretically is capable of rewarding abstraction. To provide additional information to the discriminator as to force the generator to avoid trivial solutions, embeddings for each reconstruction are obtained form an auxiliary feed-forward neural network that are also passed (along with the reconstruction) into the discriminator for generated-or-real classification.

We have not seen much success with this approach.

### Geometric Scene Similarity
Heavily inspired by Choi et al.'s paper ["Compositional Observer Communication Learning from Raw Visual Input"](https://arxiv.org/pdf/1804.02341v1.pdf){:target="_blank"}, the geometric scene similarity task is the most successful one to date, both philosophically and pragmatically. 

---

## Models

### Dual Listener-Speaker Model

### Language Siamese Model

---

## Properties of Language

This section describes various properties of language we hope to see demonstrated by the created synthetic language. The first three ({discrete, sequential, variable-length}) are explicitly built into the design of the architecture, the remainder are metrics that may have auxiliary or indirect effect on the model.

### Discrete

### Sequential

### Variable-Length

### Grounded

### Compositional




